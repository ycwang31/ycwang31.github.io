<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yongcong Wang</title>

    <meta name="author" content="Yongcong Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yongcong Wang
                </p>
                <p>I'm a second year undergraduate student from <a href="https://it.njfu.edu.cn/">School of Information Science and Technology, School of Artificial Intelligence</a> ,<a href="https://www.njfu.edu.cn/">Nanjing Forestry University</a>. My research interest includes computer vision, computer graphics, machine learning, and computational photography..
                </p>
                <!--<p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p>-->
                <p style="text-align:center">
                  <a href="mailto:ycwang1031@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="images/wechat.jpg">WeChat</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ycwang31">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul>
                <li>[Oct 2023] Passed my PhD Qualifying Exam! </li>
                <li>[Jun 2023] "The Design of a Virtual Prototyping System for Authoring Interactive VR Environments from Real World Scans" is accepted by JCISE.</li>
                <li>[Feb 2023] "The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects" is accepted by CVPR'23.</li>
                <li>[Jan 2023] "Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear" is accepted by ICRA'23.</li>
                <li>[Aug 2022] "See, Hear, Feel: Smart Sensory Fusion for Robotic Manipulation" is accepted by CoRL'22.</li>
                <li>[Sep 2021] Started at Stanford as a Msc student in Mechanical Engineering.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm broadly interested in artificial intelligence and robotics, including but not limited to perception, planning, control, hardware design, and human-centered AI. 
                The goal of my research is to build agents that can achieve human-level of learning and adapt to novel and challenging scenarios by leveraging multisensory information including vision, audio, touch, etc.
                <!-- Recently, I am interested in modeling dynamics with tactile information to learn a generalizable representation for fine-grained and effective manipulation tasks. Also, I'm facinated with tactile sensor design and would love to build robust, powerful, and low-cost sensors for robotic applications. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:30%;vertical-align:top">
        <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457"> <img style="width:107%;max-width:107%" src="images/VRDesign.png" class="hoverZoomLink">
        </a>
      </td>
      <td style="padding:20px;width:70%;vertical-align:top">
        <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">
        <!-- heading -->
        <papertitle>The Design of a Virtual Prototyping System for Authoring Interactive VR Environments from Real World Scans</papertitle>
        </a>
        <!-- authors -->
        <br>
        <a href="https://engineering.purdue.edu/cdesign/wp/author/ananya/">Ananya Ipsita</a>*,
        Runlin Duan*,
        <strong>Hao Li</strong>*,
        <a href="https://schidamb.github.io/">Subramanian Chidambaram</a>,
        <a href="https://www.yuanzhi-cao.com/">Yuanzhi Cao</a>,
        Min Liu,
        Alexander J Quinn,
        <a href=https://scholar.google.com/citations?user=AUPTVF0AAAAJ&hl=en>Karthik Ramani</a>
        <note>(*Equal Contribution)</note>
        <!-- conference & date -->
        <br>
        <em>Journal of Computing and Information Science in Engineering (JCISE)</em>
        <br>
        <!-- links -->
        <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">arXiv</a>
        <p></p>
        <p>Using our VRFromX system, we performed a usability evaluation with 20 DUs from which 12 were novices in VR programming with a welding use case. </p>
        <!-- / <a href="TBD">arXiv</a> -->
      </td>
    </tr>
          
    <tr>
      <td style="padding:20px;width:30%;vertical-align:top">
        <a href="https://objectfolder.stanford.edu/"><video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
      <source src="data/obj_folder.mp4" type="video/mp4" />
      </video></a>
      </td>
      <td style="padding:20px;width:70%;vertical-align:top">
        <a href="https://objectfolder.stanford.edu/">
        <!-- heading -->
        <papertitle>The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects</papertitle>
        </a>
        <!-- authors -->
        <br>
        <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>*,
        <a href="https://dou-yiming.github.io/">Yiming Dou</a>*,
        <strong>Hao Li</strong>*,
        <a href="http://tanmay-agarwal.com/">Tanmay Agarwal</a>,
        <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
        <a href="https://yunzhuli.github.io/"> Yunzhu Li</a>,
        <a href=https://profiles.stanford.edu/fei-fei-li>Li Fei-Fei</a>,
        <a href=https://jiajunwu.com>Jiajun Wu</a>
        <note>(*Equal Contribution)</note>
        <!-- conference & date -->
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
        <br>
        <!-- links -->
        <a href="https://www.objectfolder.org/swan_vis/">dataset demo</a>
        / <a href="https://objectfolder.stanford.edu/">project page</a>
        / <a href="https://arxiv.org/pdf/2306.00956.pdf">arXiv</a>
        <p></p>
        <p>We introduce the OBJECTFOLDER BENCHMARK, a
          benchmark suite of 10 tasks for multisensory object-centric
          learning, and the OBJECTFOLDER REAL dataset, in-
          cluding the multisensory measurements for 100 real-world
          household objects. </p>
        <!-- / <a href="TBD">arXiv</a> -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:30%;vertical-align:top">
        <a href="https://ai.stanford.edu/~rhgao/sonicverse/"><video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
      <source src="data/sonicverse.mp4" type="video/mp4" />
      </video></a>
      </td>
      <td style="padding:20px;width:70%;vertical-align:top">
        <a href="https://ai.stanford.edu/~rhgao/sonicverse/">
        <!-- heading -->
        <papertitle>Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear</papertitle>
        </a>
        <!-- authors -->
        <br>
        <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>*,
        <strong>Hao Li</strong>*,
        Gokul Dharan,
        Zhuzhu Wang,
        <a href="https://www.chengshuli.me/">Chengshu Li</a>,
        <a href="https://fxia22.github.io">Fei Xia</a>,
        <a href="https://scholar.google.com/citations?user=ImpbxLsAAAAJ&hl=en">Silvio Savarese</a>,
        <a href=https://profiles.stanford.edu/fei-fei-li>Li Fei-Fei</a>,
        <a href=https://jiajunwu.com>Jiajun Wu</a>
        <note>(*Equal Contribution, in alphabetical order)</note>
        <!-- conference & date -->
        <br>
        <em>International Conference on Robotics and Automation (ICRA)</em>, 2023
        <br>
        <!-- links -->
        <a href="https://ai.stanford.edu/~rhgao/sonicverse/">project page</a>
        / <a href="https://arxiv.org/pdf/2306.00923.pdf">arXiv</a>
        <p></p>
        <p>We introduce SONICVERSE, a multisensory
          simulation platform with integrated audio-visual simulation
          for training household agents that can both see and hear.
          We demonstrate SONICVERSE’s realism via sim-to-real
          transfer.</p>
        <!-- / <a href="TBD">arXiv</a> -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:30%;vertical-align:top">
        <a href="https://ai.stanford.edu/~rhgao/see_hear_feel/"><video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
      <source src="data/multisensory.mp4" type="video/mp4" />
      </video></a>
      </td>
      <td style="padding:20px;width:70%;vertical-align:top">
        <a href="https://ai.stanford.edu/~rhgao/see_hear_feel/">
        <!-- heading -->
        <papertitle>See, Hear, Feel: Smart Sensory Fusion for Robotic Manipulation</papertitle>
        </a>
        <!-- authors -->
        <br>
        <strong>Hao Li</strong>*,
        Yizhi Zhang*,
        <a href="https://josephzhu.com/">Junzhe Zhu</a>,
        <a href="https://shaoxiongwang.com/">Shaoxiong Wang</a>,
        <a href="https://scholar.google.com/citations?user=2Dmb3XYAAAAJ&hl=en">Michelle A. Lee</a>,
        <a href="http://hxu.rocks/">Huazhe Xu</a>,
        <a href="http://persci.mit.edu/people/adelson">Edward Adelson</a>,
        <a href=https://profiles.stanford.edu/fei-fei-li>Li Fei-Fei</a>,
        <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>†,
        <a href=https://jiajunwu.com>Jiajun Wu</a>†
        <note>(*Equal Contribution)</note>
        <note>(†Equal Advising)</note>
        <!-- conference & date -->
        <br>
        <em>Conference on Robot Learning (CoRL)</em>, 2022
        <br>
        <!-- links -->
        <a href="https://ai.stanford.edu/~rhgao/see_hear_feel/">project page</a>
        / <a href="https://arxiv.org/pdf/2212.03858.pdf">arXiv</a>
        <p></p>
        <p>We build a robot system that can see with a camera, 
          hear with a contact microphone, and feel with a vision-based tactile sensor, 
          with all three sensory modalities fused with a self-attention model.</p>
        <!-- / <a href="TBD">arXiv</a> -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:30%;vertical-align:top">
        <a href="https://engineering.purdue.edu/cdesign/wp/vrfromx-from-scanned-reality-to-interactive-virtual-experience-with-human-in-the-loop/"><video id="v0" width="100%" preload="auto" playsinline="" muted="" loop="" autoplay="" style="width:107%;max-width:107%">
      <source src="data/VRFromX.mp4" type="video/mp4" />
      </video></a>
      </td>
      <td style="padding:20px;width:70%;vertical-align:top">
        <a href="https://engineering.purdue.edu/cdesign/wp/vrfromx-from-scanned-reality-to-interactive-virtual-experience-with-human-in-the-loop/">
        <!-- heading -->
        <papertitle>VRFromX: From Scanned Reality to Interactive Virtual Experience with Human-in-the-Loop</papertitle>
        </a>
        <!-- authors -->
        <br>
        <a href="https://engineering.purdue.edu/cdesign/wp/author/ananya/">Ananya Ipsita</a>,
        <strong>Hao Li</strong>,
        Runlin Duan,
        <a href="https://www.yuanzhi-cao.com/">Yuanzhi Cao</a>,
        <a href="https://schidamb.github.io/">Subramanian Chidambaram</a>,
        Min Liu,
        <a href=https://scholar.google.com/citations?user=AUPTVF0AAAAJ&hl=en>Karthik Ramani</a>
        <!-- conference & date -->
        <br>
        <em>Conference on Human Factors in Computing Systems (CHI)</em>, 2021
        <br>
        <!-- links -->
        <a href="https://engineering.purdue.edu/cdesign/wp/vrfromx-from-scanned-reality-to-interactive-virtual-experience-with-human-in-the-loop/">project page</a>
        / <a href="https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2021/06/VRFromX-1.pdf"> arXiv</a>
        / <a href="https://www.youtube.com/watch?v=27egu5VkL0M"> video</a>
        <!-- / <a href="TBD">arXiv</a> -->
        <p></p>
        <p> Using our VRFromX system,
          users can select region(s) of interest (ROI) in scanned point cloud or
          sketch in mid-air using a brush tool to retrieve virtual models and
          then attach behavioral properties to them.</p>
      </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Academic Services</h2>
              <p>
                Reviewer for CoRL, RAL, CHI
              </p>
            </td>
          </tr>
        </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Teaching</h2>
          <p>
            Course Assistant in AA274A: Principle of Robot Autonomy, Stanford University, 2022
          </p>
          <p>
            Course Assistant in CS231N: Deep Learning for Computer Vision, Stanford University, 2023
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Template from Jon Barron's website</a>
              </p>
            </td>
            <td style="text-align:right;">
              <!-- <td style="padding:300px;width:15%;vertical-align:middle"> -->
              <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=2NmscI4iZxxfuoDbbJvZzjVOT14kVLh72R9xOD8Ww8g"></script>
            </td>
          </tr>
    </tbody></table>
  </table>
</body>

</html>
